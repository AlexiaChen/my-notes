主要适合新手，比较经典的。代码一行一行搞懂。


![[Pasted image 20221006095748.png]]


首先，我们加载MNIST数据集，包含70,000张28x28的图像，显示手写数字。

你可以用Keras加载这个数据集，只需一行代码。

该函数返回分成训练集和测试集的数据集。

```python
(x_train, y_train), (x_test, y_test) = mnist.load_data()
```


x_train和x_test包含我们的训练和测试图像。

y_train和y_test包含目标值：0到9之间的数字，表示相应图像中显示的数字。

我们有60,000张图片用于训练模型，10,000张图片用于测试。

![[Pasted image 20221006100126.png]]


在处理图像时，我们需要一个有4维的张量：批处理尺寸、宽度、高度和颜色通道。

x_train是（60000，28，28）。我们需要对它进行重塑，以增加缺少的维度（"1"，因为这些图像是灰度的。）

```python
x_train = x_train.reshape((x_train.shape[0], 28, 28, 1)
```

每个像素从0到255。神经网络在较小的数值下工作得更好。

在这里，我们通过将像素除以255来使其规整化(normalize)。这样一来，每个像素就会从0到1。

```python
x_train = x_train.astype('float32') / 255.0
```

目标值从0到9（每个数字的值）。

这一行对这些值进行单热编码(one-hot encodes)。

例如，这将转换一个像5这样的值，在一个零的阵列中，有一个1对应于第五个位置。

[0, 0, 0, 0, 0, 1, 0, 0, 0, 0]

```python
y_train = to_categorical(y_train)
```


现在让我们来定义我们的模型。

在Keras中，有几种方法可以创建一个模型。这一种被称为 "序列API"。

我们的模型将是一个层的序列，我们将逐一定义。

```python
model = Sequential([...])
```

这第一行有很多事情要做。

首先，我们定义我们模型的输入形状：一个28x28x1的张量（宽度、高度、通道。）

这正是我们在训练数据集中的形状。

```python
model = Sequential([Conv2D(...,input_shape=(28,28,1),...)])
```

然后，我们定义我们的第一层：一个具有32个过滤器和3x3内核的Conv2D层。

该层将使用训练图像生成32个不同的表征。

```python
model = Sequential([Conv2D(32, (3,3), ...),...])
```

我们还需要定义该层使用的激活函数： ReLU。

你会到处看到ReLU。这是一个流行的激活函数。

它将使我们能够解决非线性问题，如识别手写数字。

```python
model = Sequential([Conv2D(..., activation='relu',...),...])
```


在我们的Conv2D层之后，我们有一个最大池化操作。

这一层的目标是对卷积层收集的信息量进行降样。

我们想扔掉不重要的细节，保留真正重要的东西。

```python
MaxPooling2D((2,2))
```

我们现在要对输出进行扁平化处理。我们希望所有的东西都是一个连续的数值列表。

这就是扁平化层的作用。它将给我们一个扁平的张量。

```python
Flatten()
```

最后，我们有几个密集层。

注意输出层的大小为10，每个可能的数字值都有一个，并有一个softmax激活。

softmax确保我们得到一个表明图像中最可能的数字的概率分布。

```python
Dense(100, activation='relu'),
Dense(10, activation='softmax')
```


在创建了我们的模型之后，我们对它进行编译。

我使用随机梯度下降（SGD）作为优化器。

损失是分类交叉熵：这是一个多类分类问题。

我们希望在模型训练的过程中记录准确率。

```python
optimizer = SGD(learning_rate=0.01, momentum=0.9)
model = compile(
	optimizer = optimizer,
	loss = 'categorical_crossentropy'，
	metrics=['accuracy']		
)
```

最后，我们拟合这个模型。这就开始训练它。

有几个注意事项。

- 我使用的是32张图片的批处理规模。
- 我总共运行了10个epochs。

当fit()完成后，我们就有了一个完全训练好的模型了。

```python
history = model.fit(x_train,y_train, epochs=10, batch_size=32)
```

现在我们来测试一下这个模型。

这将从测试集中获得一个随机的图像并显示它。

注意，我们希望图片来自测试集，包含模型在训练期间没有看到的数据。

```python
image = random.choice(x_test)

plt.imshow(image, cmap=plt.get_cmap('grap'))
plt.show()
```

我们不能忘记对图像进行重塑和归一化，就像我们之前对整个火车组所做的那样。

我这次是为我用来测试模型的图像做的。

```python
image = (image.reshape((1.28.28.1))).astype('float32') / 255.0
```

最后，我预测图像的值。

记住，结果是一个单次编码的向量。这就是为什么我取argmax值（概率最高的位置），这就是结果。

```python
digit = np.argmax(model.predict(image)[0], axis=-1)
print("prediction:", digit)
```


CNN解决MNIST数据集的完整代码:

```python
import numpy as np
import pandas as pd
import random
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Flatten, Conv2D, Dense, MaxPooling2D
from tensorflow.keras.optimizers import SGD
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.datasets import mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()

x_train = x_train.reshape((x_train.shape[0], 28, 28, 1))
x_train = x_train.astype('float32') / 255.0

y_train = to_categorical(y_train)

model = Sequential([
    Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)),
    MaxPooling2D((2, 2)),
    Flatten(),
    Dense(100, activation='relu'),
    Dense(10, activation='softmax')
])

optimizer = SGD(learning_rate=0.01, momentum=0.9)
model.compile(
    optimizer=optimizer, 
    loss='categorical_crossentropy', 
    metrics=['accuracy']
)

history = model.fit(x_train, y_train, epochs=10, batch_size=32)

image = random.choice(x_test)

plt.imshow(image, cmap=plt.get_cmap('gray'))
plt.show()

image = (image.reshape((1, 28, 28, 1))).astype('float32') / 255.0

digit = np.argmax(model.predict(image)[0], axis=-1)
print("Prediction:", digit)
```