## 工作笔记


## GB28181

[技术解码 | GB28181协议简介及实践 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/393863592)

![[Pasted image 20220726174002.png]]
![[Pasted image 20220726174017.png]]

[swwheihei/wvp-GB28181: WEB VIDEO PLATFORM是一个基于GB28181-2016标准实现的网络视频平台，负责实现核心信令与设备管理后台部分，支持NAT穿透，支持海康、大华、宇视等品牌的IPC、NVR、DVR接入。 流媒体服务基于ZLMediaKit-https://github.com/xiongziliang/ZLMediaKit 前端展示基于MediaServerUI-https://gitee.com/kkkkk5G/MediaServerUI/tree/gb28181/](https://github.com/swwheihei/wvp-GB28181)


GB28181一般要求推送PS流（PS是一种容器封装格式，与mp4 flv 等是同层次的概念，以.m2p .ps结尾），用RTP/RTCP传输。好像不是RTMP
[Difference Between MPEG-PS and MPEG-TS Explained Simply and Clearly (freevideoworkshop.com)](https://www.freevideoworkshop.com/difference-between-mpeg-ps-and-mpeg-ts/)

[MPEG program stream - Wikipedia](https://en.wikipedia.org/wiki/MPEG_program_stream)

[GB28181-Server: LiveGBS国标(GB28181)流媒体服务软件前端源码 (gitee.com)](https://gitee.com/livegbs/GB28181-Server)

[GB28181-Server/web_src at master · livegbs/GB28181-Server (github.com)](https://github.com/livegbs/GB28181-Server/tree/master/web_src)
[GB28181推流 · ZLMediaKit/ZLMediaKit Wiki (github.com)](https://github.com/ZLMediaKit/ZLMediaKit/wiki/GB28181%E6%8E%A8%E6%B5%81)

GB28181的某些实现中也是，如果同一个会话中RTP中的ssrc变化，会丢弃RTP的包
[(4条消息) RTP中SSRC_sunxiaopengsun的博客-CSDN博客_rtp ssrc](https://blog.csdn.net/sunxiaopengsun/article/details/70172090)

ZLMediaKit的运行问题已经解决
[[疑难解答] RTSP 端口554 bind 失败 · Issue #1833 · ZLMediaKit/ZLMediaKit (github.com)](https://github.com/ZLMediaKit/ZLMediaKit/issues/1833)



### RTP的PS封装

因为GB28181的推流是基于RTP的PS封装（也可以不是PS封装，比如可以直接是RTP封装MPEG-4,H.264, SVAC），所以需要了解其PS格式。

基于RTP的PS封装首先安装ISO/IEC 13818-1:2000 将音视频流封装成PS包，再将PS包封装成RTP包

进行PS封装时，应将每个视频帧封装成一个PS包，且每个关键帧的PS包中应包含系统头(System Header)和PSM(Program Stream Map)，系统头和PSM放置于PS包头后，第一个PES包之前。典型的视频关键帧PS包结构如下图，其中PESV为视频PES包，PESA为音频PES包，视频非关键帧的PS包不包含System Header和PSM。


![[7d6243a1d93bf372d9227309f2fd43e.png]]

还有PS包中的一些参数取值如下:

![[58003eca9f628ffe4dc3382281a5bfc.png]]

## RTP的音视频基本封装

该方式直接将视音频数据以负载的方式封装成 RTP包。

![[d3a367501f7ea610499383bf7200b0f.png]]

![[920217d7bd46816e69131088fea92cd.png]]

![[84af40bef47e1d1220c30330e8f0114.png]]



## RTMP & FFmpeg

[Stream MP4 video successfully to RTMP with FFMPEG - Stack Overflow](https://stackoverflow.com/questions/33883405/stream-mp4-video-successfully-to-rtmp-with-ffmpeg)


[StreamingGuide – FFmpeg](https://trac.ffmpeg.org/wiki/StreamingGuide)


## OpenCV 摄像头 & 推流

 [C++ OpenCV 顯示camera攝影機串流影像 | ShengYu Talk (shengyu7697.github.io)](https://shengyu7697.github.io/opencv-camera/)
 [How to capture video from other cameras in OpenCV using C++? (tutorialspoint.com)](https://www.tutorialspoint.com/how-to-capture-video-from-other-cameras-in-opencv-using-cplusplus)

[How to capture video from default camera in OpenCV using C++? (tutorialspoint.com)](https://www.tutorialspoint.com/how-to-capture-video-from-default-camera-in-opencv-using-cplusplus)

[Play Video from File or Camera - OpenCV Tutorial C++ (opencv-srf.com)](https://www.opencv-srf.com/2017/12/play-video-from-file-or-camera.html)

[Opencv tutorial RTMP video streaming to NGINX restream as HLS (funvision.blogspot.com)](https://funvision.blogspot.com/2022/02/video-streaming-for-machine-learning.html)

[Multimedia: Which is better: FFmpeg or GStreamer? Why? - Quora](https://www.quora.com/Multimedia-Which-is-better-FFmpeg-or-GStreamer-Why)

[streaming - How to stream via RTMP using Gstreamer? - Stack Overflow](https://stackoverflow.com/questions/41550901/how-to-stream-via-rtmp-using-gstreamer)

[python 3.x - How to output x265 compressed video with cv2.VideoWriter - Stack Overflow](https://stackoverflow.com/questions/61260182/how-to-output-x265-compressed-video-with-cv2-videowriter/61281547#61281547)

[Pipe and OpenCV to FFmpeg with audio streaming RTMP in Python - Stack Overflow](https://stackoverflow.com/questions/70471732/pipe-and-opencv-to-ffmpeg-with-audio-streaming-rtmp-in-python/70476737#70476737)

[jkuri/opencv-ffmpeg-rtmp-stream: OpenCV FFMpeg Live Video Stream over RTMP protocol. (github.com)](https://github.com/jkuri/opencv-ffmpeg-rtmp-stream)

#### OpenCV人脸检测以及人脸识别


[Face Detection on recorded videos using OpenCV in Python — Windows and macOS | by Venkatesh Chandra | Analytics Vidhya | Medium](https://medium.com/analytics-vidhya/face-detection-on-recorded-videos-using-opencv-in-python-windows-and-macos-407635c699)


[OpenCV: Face Detection in Video Capture](https://docs.opencv.org/3.4/df/d6c/tutorial_js_face_detection_camera.html)

[(4条消息) Dlib模型实现人脸识别_浮屠tufu的博客-CSDN博客_dlib人脸识别](https://blog.csdn.net/weixin_46075497/article/details/121410047)

[利用dlib库实现人脸识别_枫叶的鱼的博客-CSDN博客_dlib人脸检测](https://blog.csdn.net/w798214705/article/details/121410894)

[(4条消息) 人脸识别——基于DLib库_Mr_Nobody17的博客-CSDN博客_人脸识别dlib库](https://blog.csdn.net/Mr_Nobody17/article/details/120352866)

[(4条消息) Python调用dlib库实现人脸识别 — AI初学者快速体验人工智能实现_周雄伟的博客-CSDN博客_dlib face model](https://blog.csdn.net/ebzxw/article/details/80441556)



#### 树莓派RTMP推流测试性能

- ZLMediaKit是`Release编译`
- FFmpeg开启树莓派的`硬编码和软编码H264`推送到WSL2内的Media Server上，在windows上打开VLC播放均有延迟，RTMP通过本地局域网路由器 （`硬编码H264有5s延迟， 软编码H264有7秒延迟`）
- 用ffplay播放本地摄像头无延迟（实时性很好，0s延迟）

```bash
ffplay -showmode 0 /dev/video0
```


- FFmepg开启树莓派 `硬编码H264`，推送到在树莓派上的Media Server，用FFplay 访问 127.0.0.1播放RTMP流，也有`延迟 (5s)`
-  FFmepg开启树莓派 `软编码H264`，推送到在树莓派上的Media Server，用FFplay 访问 127.0.0.1播放RTMP流，也有`延迟 (7s)`


FFmpeg在树莓派4B上使用H264硬编码RTMP推流:

```bash
ffmpeg -re -i /dev/video0 -vcodec h264_omx -f flv rtmp://127.0.0.1:1935/live/test
```

使用FFplay在树莓派上播放RTMP流:

```bash
ffplay -showmode 0 rtmp://127.0.0.1:1935/live/test
```

以上情况，可以排除视频流的延迟不是网络延迟原因，是树莓派这边的原因，而且`能确认H264硬编码功能确实开启并且起到了提高性能的作用`了，大概三种情况:

- 树莓派这边的ZLMediaKit的性能问题 
- 还是FFmpeg编码和推流的性能问题（修改命令行的推流参数）
- 树莓派本身的设备硬件H264编码器的性能所限问题（这个无法修改，可能需要换其他硬件设备测试，比如英伟达）


1. 验证问题1：是否是ZLMediaKit的Media Server处理慢？

那么要设计一个RTMP流不经过ZLMedia Server的测试案例:

在树莓派上直接FFmpeg推流到VLC播放端:


```bash
ffmpeg -re  -i /dev/video0   -vcodec h264 -f mpegts udp://127.0.0.1:1234
```

通过UDP直接推H264流到某个端口，然后通过ffplay播放这个端口的流

```bash
ffplay -showmode 0 udp://127.0.0.1:1234
```


使用这个参数测试延迟还是7秒。

换成硬件编码:

```bash
ffmpeg -re  -i /dev/video0   -vcodec h264_omx -f mpegts udp://127.0.0.1:1234
```
但是ffplay播放不了。



设计一个经过ZLMedia server的RTMP:

```bash
ffmpeg -re -i /dev/video0 -vcodec h264 -preset ultrafast -threads 4 -f flv rtmp://127.0.0.1:1935/live/test
```

ffplay播放延迟6秒

```bash
ffmpeg -re -i /dev/video0 -vcodec h264 -preset ultrafast -f flv rtmp://127.0.0.1:1935/live/test
```

ffplay播放延迟7秒

```bash
ffmpeg -re -i /dev/video0 -vcodec h264  -f flv rtmp://127.0.0.1:1935/live/test
```

ffplay播放延迟8秒


```bash
ffmpeg -re -i /dev/video0 -vcodec h264_omx   -f flv rtmp://127.0.0.1:1935/live/test
```

ffplay播放延迟5秒


延迟相关问题:

- [怎么测试ZLMediaKit的延时？ · ZLMediaKit/ZLMediaKit Wiki (github.com)](https://github.com/ZLMediaKit/ZLMediaKit/wiki/%E6%80%8E%E4%B9%88%E6%B5%8B%E8%AF%95ZLMediaKit%E7%9A%84%E5%BB%B6%E6%97%B6%EF%BC%9F)
- [直播延时的本质 · ZLMediaKit/ZLMediaKit Wiki (github.com)](https://github.com/ZLMediaKit/ZLMediaKit/wiki/%E7%9B%B4%E6%92%AD%E5%BB%B6%E6%97%B6%E7%9A%84%E6%9C%AC%E8%B4%A8)
- [延时测试 · ZLMediaKit/ZLMediaKit Wiki (github.com)](https://github.com/ZLMediaKit/ZLMediaKit/wiki/%E5%BB%B6%E6%97%B6%E6%B5%8B%E8%AF%95)


不一定是ZLMediaKit的server问题（也可能是ZLMediakIT的原因），因为推送UDP的时候，没有打包RTMP，直接是mpegts的容器格式。播放端解码次数就减少了。

看了ZLMediaKit的文档，可能找到延迟的原因了，大部分肉眼可观测的延迟可能是播放器的原因，用ffplay要传递特殊的参数,才可以比较实时:

```bash
ffplay -showmode 0 -i rtmp://127.0.0.1:1935/live/test -fflags nobuffer
```

以上测试，几乎无延迟。在本地测试的，不走网络。

走了网络，用以上命令播放，几乎也是无延迟。又用VLC测试了一下，确定是播放器的问题了。

2. 验证问题2: 是否是ffmpeg推流的性能问题

先研究ffmpeg的推流参数，修改，观察现象。

[Encode/H.264 – FFmpeg](https://trac.ffmpeg.org/wiki/Encode/H.264)

如果不用硬件编码H264，用ffmpeg本身的libx264软编码，选择--preset 为ultrafast:

```bash
ffmpeg -re -i /dev/video0  -preset ultrafast  -vcodec libx264  -f mpegts udp://127.0.0.1:1234
```

那么通过ffplay播放，实时性很高，延迟基本在`1秒左右`。

结论，树莓派的硬件编码H264还干不过ffmpeg的软解码，这个软解码器的性能以及实时性基本达到项目的要求。但是软解码器可能CPU发热厉害，设备寿命以及耐久度会减少。



