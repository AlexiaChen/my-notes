## 统计模型基础

### 统计模型

模型是对现实世界的抽象。建模就是制作模型，统计建模就是制作统计模型。

模型有什么用呢？使用与现实世界对应的模型有助于我们理解和预测现实事物。模型要尽可能充分简单，让人类能够理解，又要充分完善，能够适当解释复杂的现实。我们要制作的就是这种面向复杂世界的简单模型。

#### 从某个角度观察复杂的现象

模型可以理解为从某个角度观察现实所得到的结果。比如气温与啤酒销售额的关系，就相当于从当天的气温这个角度观察啤酒销售额所得的模型。

这些角度都无所谓对错。

观察的切入点和建立的模型都可以随着分析母的改变而变化。

#### 数学模型

数学模型使用数学的方式来表示现象，比如列一个X与Y的关系式: $y = f(x) = 20 + 4x$

#### 概率模型

概率模型是数学模型中用概率的语言描述的模型。要想使用概率模型，就要用概率分布。

其中正态分布是比较常用的，具体使用哪种分布，还需要结合具体数据来判断。比如啤酒销售额的例子:

$$
啤酒销售额 \sim N(20 + 4*tempture, \sigma^2)
$$

上例就是啤酒销售额服从均值为第一个参数，方差为第二个参数的正态分布。

#### 统计模型

统计模型是基于数据建立的概率模型。首先要从调研入手，在制作统计模型时，要基于概率模型的结构，结合实际数据调整模型的参数。概率模型和统计模型之间的区别不大。在大多数情况下可以通用。后面提到的模型都是统计模型。

#### 概率分布与统计模型

统计模型的一个优点就是我们可以借助它明确概率分布的参数的变化规律。基于统计模型的预测结果是一个条件概率分布，也就是它参数是随着条件变化的，如之前提到的啤酒销售额与气温，气温的不同，正态分布也不同。

#### 统计模型与传统数据分析的对比

之前说到的均值差的检验等传统方法只不过是统计模型应用的一个方面。但是专注于统计模型的建模过程同样能实现对复杂现象的分析。

#### 统计模型的应用

如果在建模过程中搞错了参数，也无法得出正确的结论。如果数据本身有问题，也无法得出正确的结论。所以统计模型不过是复杂世界的简单模型而已。

当然了，统计模型让数据分析有了很大的进步，它是现代数据分析的标准框架。

### 建模方法

以构建啤酒销售额的预测模型为例。

#### 响应变量和解释变量

- 响应变量是根据某个因素而变化(响应)的变量。比如啤酒销售额就是响应变量。
- 解释变量是对关注的对象变化进行解释的变量。比如气温，天气和啤酒价格这3个变量都是解释变量。

人们使用解释变量将响应变量的变化模型化。解释变量可以有多个。响应变量叫做因变量，解释变量叫做自变量。

#### 参数模型

参数模型是尽量简化现象，用极少的参数表达的模型。

#### 非参数模型

非参数模型部追求用尽量少的参数表达模型，所以容易变得复杂。

#### 线性模型

响应变量$y$和解释变量$x$之间的关系为线性关系就是线性模型。$y = kx + b$

有限关系表面上不是线性关系，但是通过变形和简化也可以变成线性关系。

#### 系数与权重

统计模型中使用的参数叫系数

$$
啤酒销售额 \sim N(\beta_0 + \beta_1*tempture, \sigma^2)
$$

上式中，$\beta_0  \beta_1$ 分别都是系数，其中$\beta_0$叫做截距，$\beta_1$叫做回归系数。通过这些系数和气温(自变量)，可以预测参数(这里的参数是正态分布的均值)

在机器学习领域中，统计模型的系数也叫做权重。一般来说，系数为矩阵。

#### 建模=模型选择 + 参数估计

第一步就是模型选择，然后才可以进行参数估计。

"销售额随着气温变化而变化"就是模型结构。如果不加以想象，就无法对这些结构进行模型化。这里的想象就很靠经验了。

"气温升高1度，销售额就增加XXX万元"中的"XX万元"就是要估计的内容。完成参数估计后，我们就能明确气温和销售额之间的关系。如果模型预测的不准确，那么就可能是两步中的某一步出错了。

如果只使用概率分布来建模，那么参数估计这一步出错的概率就降低了，我门只要选择好模型即可。线性模型是统计模型领域中不错的入门选择。

在深度学习等机器学习的领域中，参数估计也经常出错，这时候不仅仅要掌握模型选择，还要学习许多参数估计方法。

#### 线性模型的建模方法

建模中的第二部参数估计可以交给Python自动化处理。

假设数据为线性模型，那么在更换模型结构的时，需要做:
- 改变模型的解释变量
- 改变数据的服从的分布

#### 变量选择

就是为了模型选取解释变量（自变量）。

首先把各种自变量组合在一起建模：
-  $响应变量 \sim 无解释变量$
- $响应变量 \sim A$
- $响应变量 \sim B$
- $响应变量 \sim C$
- $响应变量 \sim A + B$
- $响应变量 \sim  A + C$
- $响应变量 \sim B + C$
- $响应变量 \sim A + B + C$

比如如果响应变量是啤酒销售额，解释变量就是天气，气温，啤酒价格等。

从上述组合中选出最合适的就是变量选择。有两种方法帮助我们选择，分别是假设检验和信息两准备

#### 空模型

没有解释变量的模型叫做空模型

#### 通过假设检验选择变量

回顾上述公式 $啤酒销售额 \sim N(\beta_0 + \beta_1*tempture, \sigma^2)$  和 [[#假设检验]]

提出假设

- 零假设：解释变量的系数$\beta_1$ 为0
- 备择假设: 解释变量的系数$\beta_1$ 不为0

如果系数为0，那么就可以去掉气温的参数了，如果不为0，那么就不能忽略气温。

#### 通过信息量准则选择变量

该方法可以量化所选模型与数据的契合度。赤池信息量准则(Akaike Information Criterion， AIC)是一种常用的方法。

AIC越小，模型越合适。在进行变量选择时，我们可以基于所有可能的变量组合建立模型，再比较这些模型的AIC，最后选择AIC最小的那个模型。

#### 模型评估

完全信任所选择的模型是很危险的，所以需要评估已选择的模型。

- 评估预测精度，预测精度越高，预测的结果越好
- 检验模型是否满足建模时所假设的条件

#### 在建模之前确定分析目的

应该在编写程序之前确定分析目的，并根据这个目的收集数据和建模。

### 数据表示与模型名称

#### 正态线性模型

假设响应变量服从正态分布的线性模型叫作正态线性模型。正态线性模型属于参数模型。响应变量服从正态分布，取值范围是正负无穷大。

#### 回归分析

在正态线性模型中，解释变量为连续变量的模型叫作回归分析，也叫作回归模型。，

#### 多元回归分析

含有多个解释变量的回归分析叫作多元回归分析，一个解释变量的叫作一元回归分析。

#### 方差分析

在正态线性模型中，解释变量为分类变量的模型叫作方差分析模型。当解释变量为一个种类时，叫作一元方差分析。当解释变量为两个种类时，依次类推。

#### 广义线性模型

响应变量未必服从正态分布的线性模型叫作广义线性模型。正态线性模型也属于广义线性模型。

#### 机器学习中的叫法

在机器学习领域中，响应变量为连续变量的模型叫作回归模型，正态线性模型属于广义的回归模型。

响应变量为分类变量的模型叫作分类模型。

根据应用的概率分布，广义线性模型既可能叫作回归模型，也可能叫作分类新模型。如果总体服从二项分布，就是分类模型。总体服从泊松分布，就是回归模型。

### 参数估计-最大似然估计

#### 为什么要学习参数估计

为了深入，为了掌握

#### 似然

当参数为某值时，抽到特定样本的概率(密度)叫作似然。(Likelihood)

举个例子，假设一枚硬币，投出正面的概率为1/2， 那么1/2就是参数，上述样本的概率(似然)就是$1 \over 2$ \* *$1 \over 2$
就是 $1 \over 4$

假设投出正面的概率是1/3， 那么参数就是1/3， 上述样本的概率(似然）就是1/3 * 2/3 = 2/ 9 

对于上面的问题，参数就是投出XX的概率。

#### 似然函数

在给定参数时，计算似然的函数叫作似然函数。参数用$\theta$ 表示，似然函数记为 $\mathcal{L}(\theta)$

在掷硬币的例子中，似然函数是$\mathcal{L}(\theta) = \theta \times  (1 - \theta)$

#### 对数似然

似然取对数就是对数似然。取对数可以简化后续很多计算。

#### 对数的性质

知道了对数的性质就能理解为什么需要取对数。

- 单调递增
- 乘法转换为加法
- 绝对值不会太小

#### 最大似然法

求使得似然或对数似然的最大的参数，并且把这个参数作为参数估计值的方法就是最大似然法。

再以硬币举例子，参数为1/2时，似然为1/4。 参数为1/3 ，似然为2/9。 因为1/4 > 2/9，所以参数1/2是比较适合$\theta$的估计值。所以1/2就是硬币模型的参数估计值。

#### 最大似然估计量

通过最大似然法估计得到的参数叫作最大似然估计量。为了表明它是一个估计量，所以符号记作$\hat{\theta}$
。

#### 最大对数似然

最大似然估计量对应的对数似然叫作最大对数似然， 记作$\ln \mathcal{L}(\hat{\theta})$

#### 服从正态分布的数据的似然

如果假设啤酒销售额为变量 y，服从均值为 μ，方差为 $\sigma^2$ 的正态分布，我们可以使用最大似然估计来估计参数 μ 和 $\sigma^2$。

假设我们有一个样本容量为 2 的样本，观测到的销售额分别为 y1 和 y2。那么似然函数可以表示为：

$$
 \mathcal{L}(\mu, \sigma^2) = \mathcal{N}(y_1 | \mu,\sigma^2) \times \mathcal{N}(y_2 | \mu,\sigma^2) 
$$

接下来，求出使得$\mathcal{L}$最大的参数$\mu$和$\sigma^2$ 就可以了。

#### 多余参数

与问题没有直接关系的参数叫作多余参数。比如，以上啤酒销售额的正态分布例子，由于方差可以由均值求出，所以在最大似然法中，方差$\sigma^2$就是多余参数。只需要$\mu$ 就可以了。

#### 正态线性模型的似然计算

可以用python的`stats.norm.pdf` 函数计算。

#### 最大似然估计量的性质

当样本容量趋向于无穷大的时候，最大似然估计量的样本分布趋近于正态分布。这个性质叫作渐进正态性。在所有具有渐进正态性的估计量中，最大似然估计量的渐进方差最小，即最大似然估计量是渐进有效估计值。样本分布的方差小就表明估计量的方差和误差小。

最大似然估计在统计学和机器学习中有广泛的应用。以下是一些常见的应用领域：

1. 参数估计：最大似然估计用于估计模型中的参数。通过最大化似然函数，可以找到在给定观测数据下，最有可能产生这些数据的参数值。这在回归模型、分类模型和概率分布等各种模型中都有应用。
    
2. 假设检验：最大似然估计可以用于假设检验，用于判断一个观测数据集是否支持某个假设。通过比较不同假设下的似然函数值，可以进行假设的选择和比较。
    
3. 模型选择：最大似然估计可以用于比较不同模型的拟合优度。通过比较不同模型的似然函数值，可以选择最能解释观测数据的模型。
    
4. 参数优化：最大似然估计可以用于优化问题，例如在机器学习中的损失函数最小化问题。通过最大化似然函数，可以找到使得模型预测与观测数据最接近的参数值。
    

总之，最大似然估计是一种常用的统计推断方法，可以用于从观测数据中估计模型参数、进行假设检验和模型选择，以及优化问题等。它在许多领域中都有广泛的应用。

### 参数估计-最小化损失

这个方法与最大似然估计是一体两面的。所以也很重要。

#### 损失函数

在进行参数估计时，损失函数用于使参数最小。对于损失的定义非常关键，如果定义有问题，就不能得到合适的模型，所以应该选取适当的指标作为损失。

#### 残差

残差是响应变量的实际值与通过模型预测值之间的差(residual)。在机器学习中，这个你可以认为就是一个错误的差向量，预测数据的向量减去，模型预测出的向量。

$$
residual = y - \hat{y}
$$

#### 为什么不把残差之和作为损失指标

[[Python神经网络编程#如何更新权重]]  这里有讲，会出现正负值抵消，总和为0的错误。所以不适合作为损失指标

#### 残差的平方和

[[Python神经网络编程#如何更新权重]]  这里也有讲到，残差的平方因为使曲线更平滑的底部。

$$
残差的平方和 = \sum_{i = 1}^N({y_i - \hat{y_i}})^2
$$

既然残差的平方和合适作为损失指标，那么参数使残差的平方和最小，那么就是最合适的估计参数。

#### 最小二乘法

求使得残差平方的和的最小的参数，并把这个参数作为参数估计值的方法就是最小二乘法。另一种说法是，最小二乘法(OLS)是以残差平方和为损失指标，求使得损失最小的参数的方法。

#### 最小二乘法与最大似然法的关系

最小二乘法得到的参数估计值等于假设总体服从正态分布时最大似然法的结果(估计值)。所以最小二乘法在实践中是一种非常高效的方法，为了简化流程，我们应该尽量使用最小二乘法。

#### 误差函数

在机器学习领域中，误差函数就是对数似然的相反数。求最小的误差函数，就相当于求最大似然。因为最小损失等于最大似然嘛，前面已经说过。

于是，最小二乘法还可以解释为，假设总体服从正态分布时，让误差函数最小。

#### 多种损失函数

在正态线性模型中，如果以残差平方和作为损失指标，最小损失法和最大似然法得到的参数估计值就会相等。

如果总体不服从正态分布，二者的估计值就会不同了。服从二项分布的总体，就不适合作为损失函数。我们应该根据数据选择损失函数。

### 预测精度的评估与变量选择



## 正态线性模型

## 广义线性模型

## 统计学与机器学习