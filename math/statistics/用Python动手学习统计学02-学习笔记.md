## 统计模型基础

### 统计模型

模型是对现实世界的抽象。建模就是制作模型，统计建模就是制作统计模型。

模型有什么用呢？使用与现实世界对应的模型有助于我们理解和预测现实事物。模型要尽可能充分简单，让人类能够理解，又要充分完善，能够适当解释复杂的现实。我们要制作的就是这种面向复杂世界的简单模型。

#### 从某个角度观察复杂的现象

模型可以理解为从某个角度观察现实所得到的结果。比如气温与啤酒销售额的关系，就相当于从当天的气温这个角度观察啤酒销售额所得的模型。

这些角度都无所谓对错。

观察的切入点和建立的模型都可以随着分析母的改变而变化。

#### 数学模型

数学模型使用数学的方式来表示现象，比如列一个X与Y的关系式: $y = f(x) = 20 + 4x$

#### 概率模型

概率模型是数学模型中用概率的语言描述的模型。要想使用概率模型，就要用概率分布。

其中正态分布是比较常用的，具体使用哪种分布，还需要结合具体数据来判断。比如啤酒销售额的例子:

$$
啤酒销售额 \sim N(20 + 4*tempture, \sigma^2)
$$

上例就是啤酒销售额服从均值为第一个参数，方差为第二个参数的正态分布。

#### 统计模型

统计模型是基于数据建立的概率模型。首先要从调研入手，在制作统计模型时，要基于概率模型的结构，结合实际数据调整模型的参数。概率模型和统计模型之间的区别不大。在大多数情况下可以通用。后面提到的模型都是统计模型。

#### 概率分布与统计模型

统计模型的一个优点就是我们可以借助它明确概率分布的参数的变化规律。基于统计模型的预测结果是一个条件概率分布，也就是它参数是随着条件变化的，如之前提到的啤酒销售额与气温，气温的不同，正态分布也不同。

#### 统计模型与传统数据分析的对比

之前说到的均值差的检验等传统方法只不过是统计模型应用的一个方面。但是专注于统计模型的建模过程同样能实现对复杂现象的分析。

#### 统计模型的应用

如果在建模过程中搞错了参数，也无法得出正确的结论。如果数据本身有问题，也无法得出正确的结论。所以统计模型不过是复杂世界的简单模型而已。

当然了，统计模型让数据分析有了很大的进步，它是现代数据分析的标准框架。

### 建模方法

以构建啤酒销售额的预测模型为例。

#### 响应变量和解释变量

- 响应变量是根据某个因素而变化(响应)的变量。比如啤酒销售额就是响应变量。
- 解释变量是对关注的对象变化进行解释的变量。比如气温，天气和啤酒价格这3个变量都是解释变量。

人们使用解释变量将响应变量的变化模型化。解释变量可以有多个。响应变量叫做因变量，解释变量叫做自变量。

#### 参数模型

参数模型是尽量简化现象，用极少的参数表达的模型。

#### 非参数模型

非参数模型部追求用尽量少的参数表达模型，所以容易变得复杂。

#### 线性模型

响应变量$y$和解释变量$x$之间的关系为线性关系就是线性模型。$y = kx + b$

有限关系表面上不是线性关系，但是通过变形和简化也可以变成线性关系。

#### 系数与权重

统计模型中使用的参数叫系数

$$
啤酒销售额 \sim N(\beta_0 + \beta_1*tempture, \sigma^2)
$$

上式中，$\beta_0  \beta_1$ 分别都是系数，其中$\beta_0$叫做截距，$\beta_1$叫做回归系数。通过这些系数和气温(自变量)，可以预测参数(这里的参数是正态分布的均值)

在机器学习领域中，统计模型的系数也叫做权重。一般来说，系数为矩阵。

#### 建模=模型选择 + 参数估计

第一步就是模型选择，然后才可以进行参数估计。

"销售额随着气温变化而变化"就是模型结构。如果不加以想象，就无法对这些结构进行模型化。这里的想象就很靠经验了。

"气温升高1度，销售额就增加XXX万元"中的"XX万元"就是要估计的内容。完成参数估计后，我们就能明确气温和销售额之间的关系。如果模型预测的不准确，那么就可能是两步中的某一步出错了。

如果只使用概率分布来建模，那么参数估计这一步出错的概率就降低了，我门只要选择好模型即可。线性模型是统计模型领域中不错的入门选择。

在深度学习等机器学习的领域中，参数估计也经常出错，这时候不仅仅要掌握模型选择，还要学习许多参数估计方法。

#### 线性模型的建模方法

建模中的第二部参数估计可以交给Python自动化处理。

假设数据为线性模型，那么在更换模型结构的时，需要做:
- 改变模型的解释变量
- 改变数据的服从的分布

#### 变量选择

就是为了模型选取解释变量（自变量）。

首先把各种自变量组合在一起建模：
-  $响应变量 \sim 无解释变量$
- $响应变量 \sim A$
- $响应变量 \sim B$
- $响应变量 \sim C$
- $响应变量 \sim A + B$
- $响应变量 \sim  A + C$
- $响应变量 \sim B + C$
- $响应变量 \sim A + B + C$

比如如果响应变量是啤酒销售额，解释变量就是天气，气温，啤酒价格等。

从上述组合中选出最合适的就是变量选择。有两种方法帮助我们选择，分别是假设检验和信息两准备

#### 空模型

没有解释变量的模型叫做空模型

#### 通过假设检验选择变量

回顾上述公式 $啤酒销售额 \sim N(\beta_0 + \beta_1*tempture, \sigma^2)$  和 [[#假设检验]]

提出假设

- 零假设：解释变量的系数$\beta_1$ 为0
- 备择假设: 解释变量的系数$\beta_1$ 不为0

如果系数为0，那么就可以去掉气温的参数了，如果不为0，那么就不能忽略气温。

#### 通过信息量准则选择变量

该方法可以量化所选模型与数据的契合度。赤池信息量准则(Akaike Information Criterion， AIC)是一种常用的方法。

AIC越小，模型越合适。在进行变量选择时，我们可以基于所有可能的变量组合建立模型，再比较这些模型的AIC，最后选择AIC最小的那个模型。

#### 模型评估

完全信任所选择的模型是很危险的，所以需要评估已选择的模型。

- 评估预测精度，预测精度越高，预测的结果越好
- 检验模型是否满足建模时所假设的条件

#### 在建模之前确定分析目的

应该在编写程序之前确定分析目的，并根据这个目的收集数据和建模。

### 数据表示与模型名称

#### 正态线性模型

假设响应变量服从正态分布的线性模型叫作正态线性模型。正态线性模型属于参数模型。响应变量服从正态分布，取值范围是正负无穷大。

#### 回归分析

在正态线性模型中，解释变量为连续变量的模型叫作回归分析，也叫作回归模型。，

#### 多元回归分析

含有多个解释变量的回归分析叫作多元回归分析，一个解释变量的叫作一元回归分析。

#### 方差分析

在正态线性模型中，解释变量为分类变量的模型叫作方差分析模型。当解释变量为一个种类时，叫作一元方差分析。当解释变量为两个种类时，依次类推。

#### 广义线性模型

响应变量未必服从正态分布的线性模型叫作广义线性模型。正态线性模型也属于广义线性模型。

#### 机器学习中的叫法

在机器学习领域中，响应变量为连续变量的模型叫作回归模型，正态线性模型属于广义的回归模型。

响应变量为分类变量的模型叫作分类模型。

根据应用的概率分布，广义线性模型既可能叫作回归模型，也可能叫作分类新模型。如果总体服从二项分布，就是分类模型。总体服从泊松分布，就是回归模型。

### 参数估计-最大似然估计

#### 为什么要学习参数估计

为了深入，为了掌握

#### 似然

当参数为某值时，抽到特定样本的概率(密度)叫作似然。(Likelihood)

举个例子，假设一枚硬币，投出正面的概率为1/2， 那么1/2就是参数，上述样本的概率(似然)就是$1 \over 2$ \* *$1 \over 2$
就是 $1 \over 4$

假设投出正面的概率是1/3， 那么参数就是1/3， 上述样本的概率(似然）就是1/3 * 2/3 = 2/ 9 

对于上面的问题，参数就是投出XX的概率。

#### 似然函数

在给定参数时，计算似然的函数叫作似然函数。参数用$\theta$ 表示，似然函数记为 $\mathcal{L}(\theta)$

在掷硬币的例子中，似然函数是$\mathcal{L}(\theta) = \theta \times  (1 - \theta)$

#### 对数似然

似然取对数就是对数似然。取对数可以简化后续很多计算。

#### 对数的性质

知道了对数的性质就能理解为什么需要取对数。

- 单调递增
- 乘法转换为加法
- 绝对值不会太小

#### 最大似然法

求使得似然或对数似然的最大的参数，并且把这个参数作为参数估计值的方法就是最大似然法。

再以硬币举例子，参数为1/2时，似然为1/4。 参数为1/3 ，似然为2/9。 因为1/4 > 2/9，所以参数1/2是比较适合$\theta$的估计值。所以1/2就是硬币模型的参数估计值。

#### 最大似然估计量

通过最大似然法估计得到的参数叫作最大似然估计量。为了表明它是一个估计量，所以符号记作$\hat{\theta}$
。

#### 最大对数似然

最大似然估计量对应的对数似然叫作最大对数似然， 记作$\ln \mathcal{L}(\hat{\theta})$

#### 服从正态分布的数据的似然

如果假设啤酒销售额为变量 y，服从均值为 μ，方差为 $\sigma^2$ 的正态分布，我们可以使用最大似然估计来估计参数 μ 和 $\sigma^2$。

假设我们有一个样本容量为 2 的样本，观测到的销售额分别为 y1 和 y2。那么似然函数可以表示为：

$$
 \mathcal{L}(\mu, \sigma^2) = \mathcal{N}(y_1 | \mu,\sigma^2) \times \mathcal{N}(y_2 | \mu,\sigma^2) 
$$

接下来，求出使得$\mathcal{L}$最大的参数$\mu$和$\sigma^2$ 就可以了。

#### 多余参数

与问题没有直接关系的参数叫作多余参数。比如，以上啤酒销售额的正态分布例子，由于方差可以由均值求出，所以在最大似然法中，方差$\sigma^2$就是多余参数。只需要$\mu$ 就可以了。

#### 正态线性模型的似然计算

可以用python的`stats.norm.pdf` 函数计算。

#### 最大似然估计量的性质

当样本容量趋向于无穷大的时候，最大似然估计量的样本分布趋近于正态分布。这个性质叫作渐进正态性。在所有具有渐进正态性的估计量中，最大似然估计量的渐进方差最小，即最大似然估计量是渐进有效估计值。样本分布的方差小就表明估计量的方差和误差小。

最大似然估计在统计学和机器学习中有广泛的应用。以下是一些常见的应用领域：

1. 参数估计：最大似然估计用于估计模型中的参数。通过最大化似然函数，可以找到在给定观测数据下，最有可能产生这些数据的参数值。这在回归模型、分类模型和概率分布等各种模型中都有应用。
    
2. 假设检验：最大似然估计可以用于假设检验，用于判断一个观测数据集是否支持某个假设。通过比较不同假设下的似然函数值，可以进行假设的选择和比较。
    
3. 模型选择：最大似然估计可以用于比较不同模型的拟合优度。通过比较不同模型的似然函数值，可以选择最能解释观测数据的模型。
    
4. 参数优化：最大似然估计可以用于优化问题，例如在机器学习中的损失函数最小化问题。通过最大化似然函数，可以找到使得模型预测与观测数据最接近的参数值。
    

总之，最大似然估计是一种常用的统计推断方法，可以用于从观测数据中估计模型参数、进行假设检验和模型选择，以及优化问题等。它在许多领域中都有广泛的应用。

### 参数估计-最小化损失

这个方法与最大似然估计是一体两面的。所以也很重要。

#### 损失函数

在进行参数估计时，损失函数用于使参数最小。对于损失的定义非常关键，如果定义有问题，就不能得到合适的模型，所以应该选取适当的指标作为损失。

#### 残差

残差是响应变量的实际值与通过模型预测值之间的差(residual)。在机器学习中，这个你可以认为就是一个错误的差向量，预测数据的向量减去，模型预测出的向量。

$$
residual = y - \hat{y}
$$

#### 为什么不把残差之和作为损失指标

[[Python神经网络编程#如何更新权重]]  这里有讲，会出现正负值抵消，总和为0的错误。所以不适合作为损失指标

#### 残差的平方和

[[Python神经网络编程#如何更新权重]]  这里也有讲到，残差的平方因为使曲线更平滑的底部。

$$
残差的平方和 = \sum_{i = 1}^N({y_i - \hat{y_i}})^2
$$

既然残差的平方和合适作为损失指标，那么参数使残差的平方和最小，那么就是最合适的估计参数。

#### 最小二乘法

求使得残差平方的和的最小的参数，并把这个参数作为参数估计值的方法就是最小二乘法。另一种说法是，最小二乘法(OLS)是以残差平方和为损失指标，求使得损失最小的参数的方法。

#### 最小二乘法与最大似然法的关系

最小二乘法得到的参数估计值等于假设总体服从正态分布时最大似然法的结果(估计值)。所以最小二乘法在实践中是一种非常高效的方法，为了简化流程，我们应该尽量使用最小二乘法。

#### 误差函数

在机器学习领域中，误差函数就是对数似然的相反数。求最小的误差函数，就相当于求最大似然。因为最小损失等于最大似然嘛，前面已经说过。

于是，最小二乘法还可以解释为，假设总体服从正态分布时，让误差函数最小。

#### 多种损失函数

在正态线性模型中，如果以残差平方和作为损失指标，最小损失法和最大似然法得到的参数估计值就会相等。

如果总体不服从正态分布，二者的估计值就会不同了。服从二项分布的总体，就不适合作为损失函数。我们应该根据数据选择损失函数。

### 预测精度的评估与变量选择

#### 拟合精度与预测精度

- 拟合精度是模型与已知数据的契合度（有点像机器学习中的训练数据的契合度）
- 预测精度是模型与未知数据的契合度 （神经网络训练好就可以识别了，识别就是一种预测过程，契合度就是识别成功的概率）

对数似然，残差平方和等指标都可以用来表示契合度

#### 过拟合

拟合精度很高，预测精度却很低的现象叫做过拟合。模型过于契合已知数据（训练数据），就是过拟合的原因。

#### 变量选择的意义

解释变量过多是过拟合的原因。意思其实就是模型不能过于复杂，尽可能参数少，不冗余，简单直接。删除多余的解释变量有可能提高预测精度，而增加多余的解释变量会提高拟合精度。

因此，需要进行变量选择。

比如啤酒销售额的模型，就不要把3年前的暖流运动这些因素考虑进去了，虽然世界是混沌复杂的，暖流运动似乎也可能影响啤酒销售额，但是它不是主要原因。

#### 泛化误差

预测值和未知数据之间的误差叫作泛化误差

#### 训练集与测试集

用来估计参数的数据叫作训练集。  这里类似于机器学习里面的训练神经网络的训练数据集。通过训练数据，不停地调整矩阵参数。

通过评估训练集的拟合程度可以求出拟合精度，但不能评估泛化误差。

在估计参数时，特意保留的一部分已知数据叫作测试集。

使用测试集评估模型精度可以在一定程度上评估泛化误差。

#### 交叉验证

基于特定的准则把数据分为训练集和测试集，针对测试集评价模型预测精度的方法叫作交叉验证法(Cross Validation)

- 留出交叉验证是从已知数据中取出p个数据作为测试集。将其余数据作为训练集。通过这种方法计算所有可能的数据组合的预测精度，得出的结果的均值就是最终的评估值
- K折交叉验证把已知数据分为K组，取其中1组作为测试集。重复K次，以预测精度的均值作为最终的评估值

K折交叉验证的优点是更稳定可靠，对模型性能的评估更准确。它可以减少模型性能评估的方差，并且更好地利用了数据。然而，K折交叉验证的计算开销较大，因为需要训练和评估K次模型。

留出交叉验证简单易实施，计算开销小，但可能对划分的随机性较为敏感。K折交叉验证更稳定可靠，对模型性能评估更准确，但计算开销较大。在实际应用中，通常使用K折交叉验证来评估机器学习模型的性能，其中较常见的选择是5折或10折交叉验证。

#### 赤池信息量准则

数学公式如下:

$$
AIC = -2 \times (最大对数似然 - 参与估计的参数个数)
$$

^e7d897

AIC越小，模型越合适。从公式上看，最大对数似然越大，拟合精度就越高。AIC中，参数个数为惩罚指标。解释变量越多，对数似然越大，同时惩罚也越严重。AIC可以判断增加的对数似然能否弥补更多的解释变量带来的严重缺点。

我们可以用AIC删除多余的变量。比起交叉验证法，AIC的优势是计算量小。

#### 相对熵

AIC的关注点是统计模型在预测上是否优秀。统计模型的预测结果是一种概率分布，把它和数据真正服从的分布相比较，两者之间的差异性就是一个讨论重点，这个差异的指标就是相对熵。

相对熵是两个概率分布之间的伪距离，公式如下:

$$
相对熵 =\int g(x)ln(\frac{g(x)}{f(x)}​)dx
$$

$g(x)$和$f(x)$是概率密度函数。

该公式还可以化简:

$$
相对熵 =\int g(x)(\ln g(x) - \ln f(x))dx
$$

可以看到是两个概率密度分布函数的对数之差，所以可以感觉出来相对熵是衡量概率分布的差异的指标。这时g(x) 表示真实数据的概率密度，f(x)表示模型的概率密度。

相对熵（在信息论、统计学和机器学习等领域中具有广泛的应用。下面是一些实际应用的例子：

1. 模型评估：相对熵可用于评估模型的性能。通过比较真实数据分布和模型预测分布之间的相对熵，可以衡量模型与真实数据之间的差异。较低的相对熵值表示模型与真实数据分布更接近。
    
2. 特征选择：相对熵可用于选择最具信息量的特征。通过计算特征与目标变量之间的相对熵，可以确定哪些特征对目标变量的预测具有更大的贡献。
    
3. 无监督学习：相对熵可用于聚类和降维等无监督学习任务。通过最小化数据点之间的相对熵，可以将相似的数据点聚集在一起或将高维数据映射到低维空间。
    
4. 模型优化：相对熵可用于优化模型参数。通过最小化模型预测分布与真实数据分布之间的相对熵，可以找到最优的模型参数设置。
    
5. 生成模型：相对熵可用于生成模型的训练。通过最大化真实数据分布与生成模型分布之间的相对熵，可以使生成模型更好地捕捉真实数据的分布特征。
    

这些只是相对熵应用的一些例子，它在许多领域中都有广泛的应用，帮助我们理解数据分布、优化模型和进行推断等任务。

#### 最小化相对熵与平均对数似然

都希望真实分布和所预测的分布之间的差距更小，当然就是尽可能降低相对熵了。设y是响应变量，$g(y)$ 就是真实分布，$f(y)$是模型预测的分布。代入公式:

$$
\int g(y)(\ln g(y) - \ln f(y))dy
$$

化简

$$
\int (g(y)\ln g(y) - g(y)\ln f(y))dy
$$

其中，真实客观世界的概率分布$g(y)$ 是不变的，所谓为了让上式最小，只需以下公式最小:

$$
\int (- g(y)\ln f(y))dy
$$

该式的相反数就是平均对数似然。

让真实分布与所预测的分布之间的差距最小，就是让平均对数似然的相反数越小，那么，如果平均对数似然最大，真实分布与所预测的分布之间的差距就越小。

#### AIC与平均对数似然中的偏离

平均对数似然很难直接计算，因此我们经常用最大对数似然代替它。这样会有一个问题：最大对数似然有时远大于平均对数似然，所以二者有时会偏离过大。

数学上已经证明，这个偏离的大小就是参与估计的参数的个数。去掉这个偏离的结果就是AIC公式，这就是在[[#^e7d897]] 中，从最大对数似然中减去参与估计的参数的个数的原因。

#### AIC与交叉验证

如果拟合模型时，使用最大似然法，评估预测精度时使用的是对数似然，那么留出交叉验证的变量选择结果与AIC最小准则的变量选择结果趋向于相等。

这也是AIC的一个解读。使用AIC最小准则变量，有可能提高模型对未知数据的预测精度。

#### 使用AIC进行变量选择

AIC是评估模型优良度的指标，AIC越小，模型越合适。使用AIC进行变量选择，就是选用使得AIC最小的变量组合。

#### 使用变量选择代替假设检验

在分析两组数据的均值差异时，可以使用t检验判断差异是否显著，也可以使用变量选择达到目的。从真实分布中反复抽样，根据所得的样本与所预测的分布多次计算对数似然，所有结果的均值就是平均对数似然。

不过，AIC主要是为了提高预测精度而设计的指标，它和假设检验的解读方法差异很大。AIC的用途是，借助已知数据，让模型的预测精度提高。

#### 使用假设检验还是AIC

没有定论。不可以在假设检验中没有得到所预期的结果，就切换AIC。这种行为跟掩耳盗铃，p值操纵类似。

所以很多论文结果好，并不代表本身好，可能是实验所选取的方法问题。换换一种方法，可能得到的解读就不一样。这也是很多机器学习论文的结果难以复现的原因。称为玄学也不为过。

## 正态线性模型

### 含有单个连续型解释变量的模型(一元回归)

#### 环境准备

```python
import numpy
import pandas
import scipy
import matplotlib
import seaborn
import statsmodels.formula.api
import statsmodels.api

%precision 3
```

#### 读入数据并绘制图形

```python
beer = pd.read_csv('data.csv')
seaborn.jointplot(x = 'temperature', y = 'beer', data= beer, color = 'black')
```

可以从散点图中看到，大致上，气温越高，啤酒销售额也越高。

#### 建模

因为我们是单解释变量，所以我们对啤酒销售额建模:

$啤酒销售额 \sim N(\beta_0 + \beta_1 × 气温, {\sigma}^2)$

显然，这是一个正态线性模型，响应变量只有啤酒销售额。要完成这个模型，只需确定它是否需要包含气温即可。在估计参数时，只估计式中的$\beta_0$ 和$\beta_1$，忽略多余参数$\sigma^2$
#### 使用statsmodels实现模型化

接下来用代码来实现建立这个正态线性模型。

```python
import statsmodels.formula.api as smf
lm_model = smf.ols(formula = "beer - temperature", data = beer).fit()
```

代码中用了ols这个方法，ols是普通的最小二乘法的英文缩写，Ordinary Least Squares。它与假设总体服从正态分布时的最大似然法的结果相等。

定义模型结构的参数是formula。beer代码是响应变量，temperature是解释变量。相当于给定模型的解释变量和响应变量，并提供解释变量和响应变量的具体数值列表，然后这个fit方法帮你完成直到参数估计的所有过程。

#### 打印估计结果并检验系数

```python
lm_model.summary()
```

上述代码会输出模型的结果。三张表。我们只需要关注第二张表。第二张表的第一行Intercept就是线性公式的截距，就是$\beta_0$。第二行就是解释变量温度了。Coef列是系数的值，也就是斜率$\beta_1$。右边依次是系数的标准误差，t值，零假设为"系数为0"时的p值，95%置信区间的下置信界限和上置信界限。

p值很小，在三位小数点内显示为0，此时认为气温的系数$\beta_1$与0之间存在显著性差异，也就是必然存在这个系数。不然系数为0，偏离实际数据太远。

所以得知，气温的系数值，可以看到气温的系数值为0.7654为正数，说明气温越高，啤酒销售额越高。

#### 关于summary函数的输出说明

`statsmodels` 是一个用于统计建模和计量经济学的 Python 库。`summary` 方法通常用于总结回归模型的拟合结果。这个方法会返回一个表格，包含许多统计信息。以下是常见字段的解释：

1. **Dep. Variable**: 被解释变量（因变量，响应变量）的名称。
2. **Model**: 使用的模型类型（例如 OLS，即普通最小二乘）。
3. **Method**: 拟合方法（例如 Least Squares，即最小二乘法）。
4. **Date**: 生成总结的日期。
5. **Time**: 生成总结的时间。
6. **No. Observations**: 观测值的数量（样本容量）。
7. **Df Residuals**: 残差的自由度（样本容量减去估计参数的数量）。
8. **Df Model**: 模型的自由度（用到的解释变量的个数，不是模型的参数个数）。
9. **R-squared**: 决定系数，表示模型解释的总变异的比例。
10. **Adj. R-squared**: 调整后的决定系数，调整了模型自由度的影响。
11. **F-statistic**: F检验的统计量，用于检验模型整体显著性。
12. **Prob (F-statistic)**: F检验的p值。
13. **Log-Likelihood**: 对数似然值。
14. **AIC**: 赤池信息量准则，值越小模型越好。
15. **BIC**: 贝叶斯信息准则，值越小模型越好。

每个回归系数的统计信息：

1. **coef**: 回归系数的估计值。
2. **std err**: 回归系数的标准误差。
3. **t**: t统计量，用于检验回归系数是否显著。
4. **P>|t|**: t检验的p值。
5. **[0.025** : 95%置信区间的下限。
6. **0.975]**: 95%置信区间的上限。

其他诊断信息：

1. **Omnibus**: Omnibus D'Angostino's 正态性检验的统计量。
2. **Prob(Omnibus)**: Omnibus检验的p值。
3. **Skew**: 残差的偏度。
4. **Kurtosis**: 残差的峰度。
5. **Durbin-Watson**: Durbin-Watson统计量，用于检测残差的自相关性。
6. **Jarque-Bera (JB)**: Jarque-Bera检验的统计量，用于检测残差的正态性。
7. **Prob(JB)**: Jarque-Bera检验的p值。
8. **Cond. No.**: 条件数，用于检测多重共线性。

这些字段帮助我们评估模型的拟合质量和解释变量对因变量的影响。

#### 使用AIC进行模型选择

目前模型只有气温这一个解释变量，我们不妨对比一下它和空模型的AIC，

```python
null_model = smf.ols(formula = "beer - 1", data = beer).fit()
# AIC的值
if null_model.aic > lm_model.aic:
  print("will run to there")
```

空模型的AIC的值显然比有气温的模型的AIC值大，AIC的值越小，模型越契合数据。

AIC的核心是各个AIC之间的对比，其绝对值并不重要，通过相同做法计算出来的AIC的大小关系是不变的，只要不更换做法。就不会影响模型的选择。这就意味着我们要跨工具计算AIC。

> 其实也就是你不要用R语言计算的AIC和python计算出来的AIC进行比较，用相同的库来比较是正确的做法。就像在量化交易中，回测所使用的工具一定要一样，不同工具的回测结果互相比较无意义，需要一致性。

#### 回归直线

模型预测的响应变量的图形就是回归直线。当响应变量为连续变量时，它的图形叫做回归，这也是回归直线的名称来源。非线性模型预测的响应变量叫做回归曲线。

```python
seaborn.lmplot(x = "temperature", y = "beer", data = beer,
			  scatter_kws = {"color": "black"},
			  line_kws = {"color": "black"})
```

结果会同时展示数据的散点图和回归直线，阴影部分是回归直线的95%置信区间。

> 选择95%作为置信水平是因为在统计学中，95%置信区间是一种常用的平衡点，既能提供较高的置信度，又不会导致区间过宽。

#### 使用模型进行预测

#### 获取残差

#### 决定系数

#### 修正决定系数

#### 残差的直方图和散点图

#### 分位图

#### 根据summary函数的输出分析残差


## 广义线性模型

## 统计学与机器学习