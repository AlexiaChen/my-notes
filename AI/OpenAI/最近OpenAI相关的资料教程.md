
60亿参数的小型 LLM ，能够把中文写作推到这个地步，很是不容易啊。你也可以自己试试看，demo 网址在这里  [ChatGLM 6B - a Hugging Face Space by multimodalart](https://huggingface.co/spaces/multimodalart/ChatGLM-6B)

langchain crash course [LangChain Crash Course - Build apps with language models - YouTube](https://www.youtube.com/watch?v=LbT1yp6quS8)

一步一步教你用Langchain，OpenAI huggingface等工具教你搭建私有数据PDF的QA机器人 https://www.youtube.com/watch?v=IvEh7A308FU&feature=youtu.be


langchain的教程 [A Comprehensive Guide to LangChain (nathankjer.com)](https://nathankjer.com/introduction-to-langchain/)  

一个微软的研究，提高LLMs与外部数据知识的反馈能力  [Check Your Facts and Try Again: Improving Large Language Models with External Knowledge and Automated Feedback - Microsoft Research](https://www.microsoft.com/en-us/research/group/deep-learning-group/articles/check-your-facts-and-try-again-improving-large-language-models-with-external-knowledge-and-automated-feedback/)


[The LangChain Cookbook - Beginner Guide To 7 Essential Concepts - YouTube](https://www.youtube.com/watch?v=2xxziIWmaSA)

基于LLM做语义化检索的教程（langchain） [A Practical 5-Step Guide to Do Semantic Search on Your Private Data With the Help of LLMs | HackerNoon](https://hackernoon.com/a-practical-5-step-guide-to-do-semantic-search-on-your-private-data-with-the-help-of-llms)

各种LLM模型的关系图 [Ecosystem Graphs for Foundation Models (stanford.edu)](https://crfm.stanford.edu/ecosystem-graphs/index.html?mode=graph) 

各种LLM论文的关系图 [Timeline of Transformer Models / Large Language Models (AI / ML / LLM) (v-gar.de)](https://ai.v-gar.de/ml/transformer/timeline/index.html)

一个网站，用于chatWith网页的服务 [Chatbase | ChatGPT for your website](https://www.chatbase.co/)

[Turbocharge LangChain NOW: guide to 20x faster embedding | Anyscale](https://www.anyscale.com/blog/turbocharge-langchain-now-guide-to-20x-faster-embedding)

LLama模型的简史  [A brief history of LLaMA models - AGI Sphere (agi-sphere.com)](https://agi-sphere.com/llama-models/) 

[Understanding Parameter-Efficient LLM Finetuning: Prompt Tuning And Prefix Tuning (sebastianraschka.com)](https://magazine.sebastianraschka.com/p/understanding-parameter-efficient)

[St4r 在 Twitter: "最近一段时间除了评估各个LLM本地部署的效果之外，也调研了一些AGI相关的内容，比如BabyAGI/AutoGPT/AgentGPT/Generative Agents，本tweet会总结一下近期这些热门项目背后的技术原理和潜在问题。1/10" / Twitter](https://twitter.com/xinqiu_bot/status/1652572537812041728)

[Reinforcement learning course: hands-on, step by step, and free - (datamachines.xyz)](https://datamachines.xyz/the-hands-on-reinforcement-learning-course-page/)

