
é¦–å…ˆåŸºäºŽç§æœ‰æ•°æ®æž„å»ºBotçš„æµç¨‹ç›®å‰å¤§è‡´åˆ†ä¸ºä¸¤ç§ï¼Œä¸€ç§æ˜¯ä½¿ç”¨ç§æœ‰æ•°æ®é›†è¿›è¡Œfinetuneï¼Œä»Žè€Œå¾—åˆ°ç†è§£ç§æœ‰æ•°æ®çš„LLMå¤§æ¨¡åž‹ã€‚å¦ä¸€ç§æ˜¯åˆ©ç”¨æ–‡æœ¬çš„ç›¸ä¼¼åº¦æœç´¢ï¼Œæž„å»ºå‡ºcontextï¼Œåˆ©ç”¨few shot learningçš„æ–¹å¼è®©LLMè¿›è¡ŒQ&Aã€‚å‰è€…çš„æ–¹æ¡ˆä¸»è¦åŸºäºŽ [tatsu-lab/stanford_alpaca: Code and documentation to train Stanford's Alpaca models, and generate the data. (github.com)](https://github.com/tatsu-lab/stanford_alpaca)

åŸºäºŽä¸Šè¿°çš„æ–¹æ¡ˆï¼Œè¡ç”Ÿå‡ºäº†ä¸€ç³»åˆ—äº§ç‰©ï¼Œæ¯”å¦‚
[https://github.com/antimatter15/alpaca.cpp](https://t.co/tAzzVXrut3)
[https://github.com/tloen/alpaca-lora](https://t.co/jEMMZ1vtxg) 
[https://github.com/lm-sys/FastChat](https://t.co/uNzZ63uZeN) 
[https://github.com/nomic-ai/gpt4all è¿™äº›éƒ½æ˜¯åˆ©ç”¨GPT3.5/GPT4ä½œä¸ºæ•°æ®æ‰“æ ‡å¸ˆï¼Œä»Žè€Œä¸€æ–¹é¢è°ƒä¼˜Metaçš„LLaMAï¼Œå¦ä¸€æ–¹é¢é¡ºç€è¿™æ ·çš„æ€è·¯ä¹Ÿèƒ½è¿›è¡Œç§æœ‰æ•°æ®çš„finetune](https://t.co/sdoBHx5QJk)

å¦ä¸€ç§æ–¹æ¡ˆæ˜¯åˆ©ç”¨è¿™äº›å·²ç»å¯¹LLaMA finetune å¥½çš„æ¨¡åž‹è¿›è¡Œæž„å»ºã€‚æŒ‰ç…§Langchainçš„æ–‡æ¡£
https://python.langchain.com/en/latest/use_cases/question_answering.html  
å°±å¯ä»¥å¿«é€Ÿæž„å»ºï¼Œä»Žæ–‡æœ¬Embedding->å­˜å‚¨VectorDB->Similarity Search->Q&A with Contextã€‚è¿™æ—¶å€™æœ‰äººä¼šé—®ï¼ŒLangchain æ–‡æ¡£ä¸Šè¿™äº›Vectorstores è¿˜æ˜¯å¾—ç”¨åˆ°OpenAIEmbeddingsï¼Ÿé‚£æ€Žä¹ˆç§æœ‰åŒ–?

å…¶å®žç­”æ¡ˆå°±è—åœ¨ [Llama-cpp â€” ðŸ¦œðŸ”— LangChain 0.0.136](https://python.langchain.com/en/latest/modules/models/text_embedding/examples/llamacpp.html) è¿™ç¯‡æ–‡æ¡£ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥åˆ©ç”¨llama CPPè¿›è¡Œæž„å»ºEmbeddingï¼Œè¿™ä¹Ÿæ˜¯è¿‘æœŸå¼•å…¥çš„æ–°åŠŸèƒ½  [Add embedding mode with arg flag. Currently working by StrikingLoo Â· Pull Request #282 Â· ggerganov/llama.cpp (github.com)](https://github.com/ggerganov/llama.cpp/pull/282)  
è™½ç„¶ç›®å‰æ˜¯ç”¨llama.cppæž„å»ºEmbeddingï¼Œä½†æ˜¯å¯ä»¥åœ¨Q&Aä½¿ç”¨çš„ä¸Šè¿°æåˆ°çš„finetuneåŽçš„æ¨¡åž‹ï¼Œé€šè¿‡è¿™ç§æ–¹å¼ç»„åˆèµ·æ¥è¾¾åˆ°ç§æœ‰åŒ–çš„ç›®çš„ã€‚

è¿‘æœŸä¸€ç³»åˆ—llama.cppçš„ä¼˜åŒ–ï¼Œä½¿å¾—æœ¬åœ°å¯ä»¥éƒ¨ç½²è¶Šæ¥è¶Šå¤§çš„å‚æ•°æ¨¡åž‹ï¼Œæˆ‘è‡ªå·±ä½“éªŒä¸‹æ¥ï¼Œè¿™äº›æ¨¡åž‹åœ¨è‹±è¯­äº¤äº’ä¸Šæ•ˆæžœæ›´å¥½ï¼Œåœ¨ä¸­æ–‡ä¸Šè¿˜å·®äº†ä¸€äº›ï¼Œæ‰€ä»¥å¦‚æžœéœ€è¦ä½¿ç”¨ä¸­æ–‡äº¤äº’ï¼Œå¯ä»¥ä½¿ç”¨è¿™å‡ ä¸ªfinetuneæ¨¡åž‹ [ymcui/Chinese-LLaMA-Alpaca: ä¸­æ–‡LLaMA&Alpacaå¤§è¯­è¨€æ¨¡åž‹+æœ¬åœ°CPUéƒ¨ç½² (Chinese LLaMA & Alpaca LLMs) (github.com)](https://github.com/ymcui/Chinese-LLaMA-Alpaca) 
[Facico/Chinese-Vicuna: Chinese-Vicuna: A Chinese Instruction-following LLaMA-based Model â€”â€” ä¸€ä¸ªä¸­æ–‡ä½Žèµ„æºçš„llama+loraæ–¹æ¡ˆï¼Œç»“æž„å‚è€ƒalpaca (github.com)](https://github.com/Facico/Chinese-Vicuna)

è¿™é‡Œåˆ†äº«ä¸€ä¸ªç®€å•çš„å®žçŽ°ä»£ç ï¼Œé€šè¿‡ Loaderè¯»å–æ•°æ®è¿›è¡Œåˆ†è¯å¹¶æž„å»ºEmbeddingï¼ŒæŽ¥ç€æ ¹æ®é—®é¢˜è¿›è¡Œsimilarity searchï¼Œä¹‹åŽå°†åŒ¹é…åˆ°çš„ä¸Šä¸‹æ–‡å’Œé—®é¢˜ç»„åˆåœ¨ä¸€èµ·ä¸¢ç»™LLaMAè¿›è¡Œå›žç­”ã€‚ä¹‹åŽæˆ‘ä¼šå°è¯•ä½¿ç”¨ChatGLMæ­å»ºè¿™ä¸€å¥—æµç¨‹å¹¶åˆ†äº«ç»™å¤§å®¶

Creating a private data QA bot entirely using the open-source LLM project

```python
from langchain import PromptTemplate, LLMChain
from langchain.document_loaders import UnstructuredHTMLLoader
from langchain.embeddings import LlamaCppEmbeddings
from langchain.llms import LlamaCpp
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores.faiss import FAISS

loader = UnstructuredHTMLLoader("langchain/docs/_build/html/index.html")
embedding = LlamaCppEmbeddings(model_path="path/models/ggml-model-q4_0.bin")
llm = LlamaCpp(model_path="path/models/ggml-model-q4_0.bin")


def split_chunks(sources: list) -> list:
    chunks = []
    splitter = RecursiveCharacterTextSplitter(separator="", chunk_size=256, chunk_overlap=16)
    for chunk in splitter.split_documents(sources):
        chunks.append(chunk)
    return chunks


def generate_embedding(chunks: list):
    texts = [doc.page_content for doc in chunks]
    metadatas = [doc.metadata for doc in chunks]

    search_index = FAISS.from_texts(texts, embedding, metadatas=metadatas)

    return search_index


def similarity_search(
        query: str, index: FAISS
) -> (list, list):
    matched_docs = index.similarity_search(query, k=4)
    sources = []
    for doc in matched_docs:
        sources.append(
            {
                "page_content": doc.page_content,
                "metadata": doc.metadata,
            }
        )

    return matched_docs, sources


docs = loader.load()
chunks = split_chunks(docs)
embeddings = generate_embedding(chunks)

question = "What are the use cases of LangChain?"
matched_docs, sources = similarity_search(question, embeddings)

template = """
Please use the following context to answer questions.
Context: {context}
---
Question: {question}
Answer: Let's think step by step."""

context = "\n".join([doc.page_content for doc in matched_docs])
prompt = PromptTemplate(template=template, input_variables=["context", "question"]).partial(context=context)
llm_chain = LLMChain(prompt=prompt, llm=llm)

print(llm_chain.run(question))
```




## ä¸€äº›æ ·ä¾‹ 

[GPTBase](https://gptbase.ai/)

[SiteGPT â€“ ChatGPT for every website](https://sitegpt.ai/)


[Databerry - Connect your data to ChatGPT and other Language Models](https://www.databerry.ai/)


https://app.copilothub.co/

