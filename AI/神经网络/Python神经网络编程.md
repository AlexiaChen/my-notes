
又名：自己动手用Python编写神经网络

## 神经网络如何工作

### 分类器和预测期并无太大差别

分类器其实就是在一个二维坐标系中，用一条直线 $y = kx + b$ 来分割不同类别的对象。这个可以很直观地理解。

如何得到正确的斜率呢？如何才可以改进划分两种不同类别的对象的分界线呢？

### 训练简单的分类器

其实就是不停地调节斜率这个系数k。调节系数通常为称作-学习率(learning rate)。调节斜率需要适度更新，限制错误样本的影响。

### 有时候一个分类器不足以求解问题

之前提到的用一条线性分界线来分割目标是很理想的情况了，现实世界复杂多样。很多数据本身不是由单一线性过程支配。解决方法很容易，就是需要多个线性分类器来划分由一条单一直线无法分离的数据。所以人工神经网络就由此诞生了。神经网络是单一线性分类的复杂的扩展。

### 神经元-大自然的计算机

本质上就是神经元之间互相连接，形成的一种可以处理复杂信息的计算模型。神经元之间有抑制机制。所以人工神经元也有了激活函数这样的抑制神经元输出的机制，比如Sigmoid函数和ReLU函数。Sigmoid在坐标轴上是一个S曲线，正好可以模拟大脑神经元的抑制激活功能

$$
Sigmoid(x) = \frac{1}{1 + e^{-x}}
$$

人工神经元之间分层，一层有N个神经元节点，信号是一层一层地通过线性变换进行传递。上一层的每个神经元都与下一层的神经元保持全连接，但是这些连接有强有弱，所以在计算机中，用权重(weight)表示连接的强弱。

比如第二层的某个神经元，接收第一层神经元的信号，可以用数学公式表示如下:

$$
x = w_1*a + w_2*b + ... w_n*c
$$

以上公式中的a，b，c就是上一层神经元的输出信号，输出信号通过连接中的各自的权重，作为下一个神经元节点的输入信号，这个输入信号x，再经过Sigmoid函数的抑制激活作用，作为该神经元的输出信号:

$$
Sigmoid(x)
$$

通过以上，你理解了上一层所有神经元节点对下一层的某个神经元节点的输入机制，那么你后面自然就理解，在下一层所有节点的输入的表示。

$$
x_1 = w_{11}*a + w_{21}*b + ... + w_{n1}*c
$$
$$
x_2 = w_{12}*a + w_{22}*b + ... + w_{n2}*c  
$$

$$
.......
$$
$$
x_n = w_{1n}*a + w_{2n}*b + ... + w_{nn}*c  
$$


从上面的全部公式，$x_1$就表示第二层的第一个神经元的输入值，$x_2$就表示第二层的第二个神经元的输入值，以此类推。你会发现，本质上就是输入了一个$(a,b,c, ... )$的高维向量，经过一次线性变换得到一个$(x_1, x_2, ..., x_n)$的输出向量作为第二层神经元的输入。

所以显然就可以把以上的全部公式，优化成向量乘以矩阵的形式，矩阵就是线性变换的具体表示。

$$
y = Wx
$$

以上公式，x是输入向量，y是输出向量，W是权重矩阵。

以上例子只举例了两层神经网络，实际上，神经网络是有多层的。就是涉及到多次矩阵的计算。不过显而易见，对于N层的神经网络，就有N - 1次矩阵计算。所以神经网络的计算中，涉及了大量的向量和线性代数的相关计算。

神经网络中，第一层神经元和最后一层神经元分别为输入层和输出层。它们不涉及sigmoid函数的计算。其余为隐藏层。

### 如何学习

其实学习就是训练，其实就是输入训练数据集，让神经网路学习，如何具体学习呢？把数据集看成有很多$(input，output)$对 的数据表，把input数据向量化以后，变成一个N维向量，输入进第一层神经网络，最后经过一系列线性变换，吐出一个N维向量，当然，输出的N维向量不一定维度跟输入向量一致。这个输出向量再与output数据的向量进行比较。

向量如何比较？最简单就是向量求差，得到一个diff的误差向量$e$，误差向量再通过反向传播，也就是乘以之前权重矩阵的转置矩阵$W^T$，进行反向计算。

$$
error = W^Te
$$

最重要地来了，反向传播就是为了为以后的权重做更新，也就是对权重矩阵$W$ 做修改，不停地修改，不停地调整权重到最合适的数值，那么就算学习的过程了。

回顾一下，我们之前提到的误差，误差是怎么来的，无非是神经网络的输出的向量与训练数据集的向量集合做对比，以训练数据集为参考。我们的神经网络要通过一次又一次地与训练数据的向量做对比，调整权重，达到最优。本质就是在一次又一次地对比中，不停地减小这个误差。

为了在一次学习中，寻找最小的误差，就需要用到梯度下降(gradient descent)的思想，找到最小的y值。也就是函数的最低的谷底。但是由于神经网络中的权重，本身是一个复杂的数学函数，影响y的输出的因素太多了，所以我们是在一个高维的空间上，寻找y的最小值，并不是一个简单的二维函数坐标。比如，如果依赖两个参数的函数，其本质就是一个三维空间的图像。以此类推。

![[Pasted image 20230510155740.png]]



### 如何更新权重

神经网络的输出是一个极其复杂的函数，这个函数有许多参数影响其输出的连接的权重，我们可以使用之前提到的梯度下降法，计算出正确的权重吗？只要我们使用合适的误差函数，这是完全可以的。

因为神经网络的输出，本身不是误差值，我们是用神经网络的输出，与训练数据的目标值作差。这个是最简单的误差。

- 训练目标值 - 实际网络输出值
- |  训练目标值 - 实际网路输出值 |
- (训练目标值 - 实际网路输出值)^2

上面三种都是误差函数。第一个误差函数最简单，但是有可能对误差值求和，正负的值相互抵消，甚至最极端的情况，总和为0，造成错误。第二个误差函数改进了这点，用绝对值，这样就不管正负了，避免了符号影响，但是还记得这样的函数在二维坐标是如何的吗？|x - 5| = y

![[Pasted image 20230510161029.png]]

是一个V型的翻折函数。这种函数，梯度下降，因为是小幅度地改变x的值来接近最低点，遇到这种函数就会反复地在最低点之间来回跳动优化，所以这个函数没有得到广泛使用。而且，即使接近了最小值，斜率也不会变得更小，也有超调的风险。

所以最后一种误差函数最好，比如 $y = (x - 5)^2$

![[Pasted image 20230510161430.png]]

使用误差的平方，可以方便使用代数计算出梯度下降的斜率。误差函数平滑连续，没有间断和跳跃。越接近最小值，梯度越小。意味着使用这个函数调节步长，超调的风险会变得更小。

y是误差。

使用梯度下降，需要计算出误差函数相对于权重的斜率，误差函数的输入是神经网路的输出，所以误差函数是很依赖神经网络的权重的，我们最终要找到最小的y值，所对应的权重矩阵。